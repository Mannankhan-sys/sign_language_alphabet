# ğŸ¤š Sign Language Alphabet Detection System - ULTIMATE VERSION

A professional real-time sign language alphabet recognition system using **MediaPipe** hand tracking and **LSTM neural networks**. Detects A-Z alphabet gestures + SPACE and BACKSPACE controls via webcam and builds sentences letter-by-letter.

![Python](https://img.shields.io/badge/Python-3.12-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.16-orange)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10.14-green)
![Accuracy](https://img.shields.io/badge/Accuracy-92.65%25-success)

---

## ğŸ¯ Key Features

### Detection Capabilities
- **âœ… A-Z Alphabet Letters**: Complete English alphabet recognition
- **âœ… SPACE Gesture**: Add spaces between words (open palm)
- **âœ… BACKSPACE Gesture**: Delete last character (swipe left)
- **âš ï¸ PERIOD Disabled**: Low accuracy (43%) - use keyboard instead

### Advanced Technology
- **ğŸ¯ Stable Detection System**: Requires 10-15 consecutive matching predictions
- **ğŸ“Š Dual Confidence Thresholds**: 85% for letters, 90% for controls
- **ğŸ¨ Professional HD UI**: 1280x720 with dark theme
- **ğŸ“ˆ Real-time Progress Bars**: Visual feedback for detection stability
- **ğŸ”„ Detection History**: Shows last 6 recognized signs with confidence scores
- **âš¡ Flash Effects**: Visual confirmation for special gestures
- **ğŸ¯ Hand Guide Box**: Positioning aid for consistent detection
- **ğŸ“± Color-Coded Gestures**: Different colors for letters vs controls

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Webcam  â”‚â”€â”€â”€â–¶â”‚   MediaPipe   â”‚â”€â”€â”€â–¶â”‚   LSTM   â”‚â”€â”€â”€â–¶â”‚   Detected   â”‚
â”‚   Feed   â”‚    â”‚ Hand Tracking â”‚    â”‚   Model  â”‚    â”‚ Letter/Actionâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                    â”‚
                       â–¼                    â–¼
                21 Landmarks         30-Frame Sequence
                (x,y,z) Ã— 21         (30 Ã— 126 values)
```

### Detection Pipeline

1. **Webcam Input**: 30 FPS video capture
2. **Hand Detection**: MediaPipe extracts 21 3D landmarks per frame  
3. **Sequence Building**: Collects 30 consecutive frames (~1 second)
4. **LSTM Prediction**: Classifies sequence as A-Z or control gesture
5. **Stability Voting**: Sign must appear 10-15 times consecutively
6. **Action Execution**: Letter added or control action performed

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.12
- Webcam (built-in or USB)
- Windows 10/11, Linux, or macOS

### Installation

```bash
# 1. Navigate to project folder
cd C:\MyWorks\projects\sign_language_alphabet

# 2. Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# 3. Install dependencies (IN THIS ORDER - critical!)
pip install numpy==1.26.4
pip install opencv-python==4.9.0.80
pip install mediapipe==0.10.14
pip install tensorflow==2.16.1
pip install scikit-learn

# 4. Verify installation
python -c "import cv2, mediapipe, tensorflow; print('âœ“ All packages ready!')"
```

### Run the Ultimate Detector

```bash
python 4_alphabet_detector_ultimate.py
```

---

## ğŸ“– Complete Usage Guide

### Understanding the Interface

**TOP PANEL:**
- Title and status
- Hand detection indicator (green = detected)
- FPS counter

**CENTER:**
- Green guide box with animated corners
- Live camera feed
- Hand landmark visualization (green dots + blue lines)

**RIGHT PANEL:**
- Current detection with sign type ([LETTER] or [CONTROL])
- Confidence percentage
- Hand Visibility progress bar (22/22 frames)
- Sign Stability progress bar (10-15/10-15 predictions)
- Recent history (last 6 detections with fade effect)

**BOTTOM LEFT:**
- Special gestures legend with descriptions
- Color-coded indicators

**BOTTOM CENTER:**
- Sentence output
- Word-wrapped display (68 chars per line)

### Step-by-Step Usage

**1. Position Your Hand**
```
âœ“ Place hand inside green guide box
âœ“ Keep hand centered in frame
âœ“ Distance: 1-2 feet from camera
âœ“ Ensure good lighting
```

**2. Make a Sign**
```
âœ“ Form clear alphabet letter (A-Z)
âœ“ OR make special gesture:
  - SPACE: Open palm facing camera
  - BACKSPACE: Swipe hand left
âœ“ Hold position steady (don't move!)
```

**3. Watch Progress Bars**
```
Hand Visibility Bar:
  â””â”€ Must reach 100% (22/22 frames)
  
Sign Stability Bar:
  â””â”€ Letters: 100% (10/10 predictions)
  â””â”€ Controls: 100% (15/15 predictions for higher accuracy)
```

**4. Wait for Detection**
```
âœ“ When both bars fill, sign is recognized
âœ“ Letter appears in sentence
âœ“ OR control action executes:
  - SPACE: Flash blue + space added
  - BACKSPACE: Flash orange + char deleted
âœ“ Console prints confirmation
```

**5. Continue Spelling**
```
âœ“ REMOVE hand completely
âœ“ Wait 1 second (cooldown)
âœ“ Show next letter
âœ“ Repeat process
```

### Keyboard Shortcuts

| Key | Action | Notes |
|-----|--------|-------|
| `Q` | Quit app | Or press ESC |
| `C` | Clear sentence | Resets everything |
| `SPACE` | Add space | Backup if gesture fails |
| `BACKSPACE` | Delete char | Backup if gesture fails |

---

## ğŸ¯ Gesture Recognition Guide

### Alphabet Letters (A-Z)

**High-Accuracy Letters (>95%):**
- **E, F, J, K, P, Q, R, X, Z**: Very distinct hand shapes

**Good Letters (85-95%):**
- **A, B, D, G, H, L, O, S, U, V, W**: Clear finger configurations

**Challenging Letters (<85%):**
- **C, N, M, T, Y**: Similar shapes or low training samples
- *Tip*: Exaggerate these gestures and hold extra steady

### Special Gestures

**SPACE (86% accuracy):**
```
Gesture: Open palm facing camera
Fingers: All extended and spread
Thumb: Extended outward
Duration: Hold 2 seconds
Visual: Blue flash on success
```

**BACKSPACE (57% accuracy - use with caution):**
```
Gesture: Swipe hand left
Motion: Start right, sweep left quickly
Fingers: Can be relaxed
Duration: Quick motion
Visual: Orange flash on success
Fallback: Use keyboard BACKSPACE if fails
```

**PERIOD (Disabled - 43% accuracy):**
```
Status: Not available via gesture
Reason: Too similar to other signs
Solution: Type period manually or add to sentence later
```

---

## ğŸ“Š Model Performance

### Overall Metrics
```
Dataset:          1,850 samples
Training Split:   80% train, 20% test
Architecture:     LSTM (128â†’64) + Dense layers
Epochs:           50
Test Accuracy:    92.65%
Test Loss:        0.2826
```

### Per-Category Performance

**Alphabet Letters:**
| Category | Accuracy | Count | Examples |
|----------|----------|-------|----------|
| Excellent | >95% | 11 letters | E, F, I, J, K, M, P, Q, R, X, Z |
| Good | 85-95% | 12 letters | A, B, D, G, H, L, O, S, U, V, W |
| Fair | 70-85% | 3 letters | C, N, T, Y |

**Control Gestures:**
| Gesture | Precision | Recall | F1-Score | Enabled? |
|---------|-----------|--------|----------|----------|
| SPACE | 86% | 100% | 0.92 | âœ… Yes |
| BACKSPACE | 57% | 67% | 0.62 | âœ… Yes (with caution) |
| PERIOD | 43% | 100% | 0.60 | âŒ No |

### Known Limitations & Solutions

**Issue**: Similar letters confused (Nâ†”M, Uâ†”V, Câ†”O)  
**Cause**: Very similar hand configurations  
**Solution**: Exaggerate differences, hold longer

**Issue**: BACKSPACE unreliable (57% precision)  
**Cause**: Low sample count (only 6 test samples)  
**Solution**: Record 50+ more samples, or use keyboard backup

**Issue**: Some letters have low samples (C=10, E=7, M=3)  
**Cause**: Kaggle dataset imbalance  
**Solution**: Record additional samples using `record_Q.py` template

---

## ğŸ› Troubleshooting

### Camera Issues

**"Camera not accessible"**
```bash
Solutions:
1. Check Windows camera permissions
2. Close other camera apps (Zoom, Teams)
3. Try different camera index:
   cap = cv2.VideoCapture(1)  # Change from 0
4. Restart computer
```

### Hand Not Detected

**Green indicator shows "NO HAND"**
```bash
Solutions:
1. Improve lighting - avoid backlighting
2. Use plain background (not skin tone)
3. Move closer (1-1.5 feet optimal)
4. Clean camera lens
5. Lower detection threshold:
   min_detection_confidence=0.3
```

### False Detections / Jittery

**Random letters appearing**
```bash
Already Implemented:
âœ“ 10-prediction voting system
âœ“ 22/30 hand visibility requirement
âœ“ 3-second cooldown between same letters

Additional Fixes:
1. Raise confidence:
   CONFIDENCE_THRESHOLD = 0.90
2. Increase stability:
   STABLE_PREDICTIONS_REQUIRED = 15
3. Keep hand completely still
```

### Model Not Found

**"Model not found: alphabet_model.keras"**
```bash
Solutions:
1. Check you're in correct directory:
   cd C:\MyWorks\projects\sign_language_alphabet
   
2. Run training first:
   python 1_prepare_alphabet_data.py
   python 2_record_special_signs.py
   python 3_train_alphabet_model.py
```

### Installation Errors

**NumPy version conflicts**
```bash
pip uninstall numpy
pip install numpy==1.26.4 --force-reinstall --no-deps
```

**TensorFlow DLL errors (Windows)**
```bash
pip uninstall tensorflow
pip install tensorflow==2.16.1 --no-cache-dir
```

**MediaPipe import error**
```bash
# Remove conflicting package
pip uninstall opencv-contrib-python
# Keep only
pip install opencv-python==4.9.0.80
```

---

## ğŸ“ Project Structure

```
sign_language_alphabet/
â”‚
â”œâ”€â”€ Data Preparation
â”‚   â”œâ”€â”€ 1_prepare_alphabet_data.py    # Extract landmarks from Kaggle dataset
â”‚   â””â”€â”€ 2_record_special_signs.py     # Record SPACE/BACKSPACE/PERIOD
â”‚
â”œâ”€â”€ Model Training
â”‚   â”œâ”€â”€ 3_train_alphabet_model.py     # Train LSTM model
â”‚   â””â”€â”€ alphabet_training_report.txt  # Training metrics (generated)
â”‚
â”œâ”€â”€ Detection Applications
â”‚   â”œâ”€â”€ 4_alphabet_detector_pro.py     # Professional version (letters only)
â”‚   â””â”€â”€ 4_alphabet_detector_ultimate.py # Ultimate version (letters + controls)
â”‚
â”œâ”€â”€ Model Files (Generated)
â”‚   â”œâ”€â”€ alphabet_model.keras           # Trained LSTM model
â”‚   â””â”€â”€ alphabet_labels.json           # Label-to-index mapping
â”‚
â”œâ”€â”€ Dataset (Generated)
â”‚   â””â”€â”€ alphabet_dataset/
â”‚       â”œâ”€â”€ A/ ... Z/                  # Landmark sequences for letters
â”‚       â”œâ”€â”€ SPACE/                     # Space gesture samples
â”‚       â”œâ”€â”€ BACKSPACE/                 # Backspace gesture samples
â”‚       â””â”€â”€ PERIOD/                    # Period gesture samples
â”‚
â””â”€â”€ Documentation
    â”œâ”€â”€ README.md                      # This file
    â””â”€â”€ requirements.txt               # Python dependencies
```

---

## ğŸ”¬ Technical Deep Dive

### LSTM Model Architecture

```python
Sequential([
    Input(shape=(30, 126)),           # 30 frames Ã— 126 features
    
    LSTM(128, return_sequences=True), # First LSTM layer
    Dropout(0.3),
    
    LSTM(64, return_sequences=False), # Second LSTM layer
    Dropout(0.3),
    
    Dense(64, activation='relu'),     # Fully connected
    Dropout(0.2),
    
    Dense(29, activation='softmax')   # Output: 26 + 3 controls
])

Optimizer: Adam (lr=0.001)
Loss: Sparse Categorical Crossentropy
Metrics: Accuracy
```

**Why LSTM?**
- Captures temporal patterns in gesture sequences
- Better than single-frame CNN for motion-based signs
- Handles variable-length movements within fixed window

### Feature Engineering

**Input Shape**: (30, 126)
- **30 frames**: 1 second of data at 30 FPS
- **126 features per frame**:
  - Left hand: 21 landmarks Ã— 3 coordinates = 63 values
  - Right hand: 21 landmarks Ã— 3 coordinates = 63 values
  - Total: 126 values

**Landmark Coordinates**:
```
Hand landmarks (0-20):
  0: Wrist
  1-4: Thumb (CMC, MCP, IP, TIP)
  5-8: Index finger
  9-12: Middle finger
  13-16: Ring finger
  17-20: Pinky finger

Each landmark: (x, y, z)
  x: Normalized 0-1 (left to right)
  y: Normalized 0-1 (top to bottom)
  z: Depth relative to wrist
```

### Dual Confidence Threshold System

```python
if sign in SPECIAL_GESTURES:
    threshold = 0.90  # Higher for controls
    stable_frames = 15
else:
    threshold = 0.85  # Standard for letters
    stable_frames = 10
```

**Rationale**:
- Special gestures have lower precision (57-86%)
- Require more evidence before triggering
- Prevents accidental deletions/spaces

---

## ğŸš€ Advanced Features

### 1. Flash Effect System
```python
# Visual confirmation for special gestures
if gesture == "SPACE":
    flash_color = BLUE
elif gesture == "BACKSPACE":
    flash_color = ORANGE
    
# Flash duration: 0.3 seconds
# Applied to guide box borders
```

### 2. Color Coding
- **Green**: Alphabet letters
- **Blue**: SPACE gesture
- **Orange**: BACKSPACE gesture
- **Purple**: PERIOD gesture (disabled)

### 3. Detection History
- Stores last 6 detections
- Fades older entries (alpha blending)
- Shows confidence scores
- Helps debug false positives

### 4. Adaptive Cooldown
```python
# Prevent duplicate detections
if same_sign and time_since_last < 3.5:
    ignore_prediction()
```

---

## ğŸ“ˆ Future Improvements

### Short-term (1-2 weeks)
- [ ] Record 50+ more BACKSPACE samples â†’ improve to 80%+
- [ ] Add sound effects for detections
- [ ] Export sentence to text file
- [ ] Add undo/redo functionality

### Medium-term (1 month)
- [ ] Word prediction/auto-complete
- [ ] Common phrase shortcuts
- [ ] Multi-language support (ASL, BSL, PSL)
- [ ] Mobile app (Android/iOS)

### Long-term (3+ months)
- [ ] Two-hand gestures
- [ ] Dynamic signs (motion-based)
- [ ] Real-time grammar correction
- [ ] Cloud API deployment
- [ ] Integration with messaging apps

---

## ğŸ¯ Tips for Best Performance

### Environment Setup
âœ… **DO:**
- Use bright, even lighting
- Plain background (white wall ideal)
- Stable camera position
- Minimal movement while signing

âŒ **DON'T:**
- Backlit windows behind you
- Busy/patterned background
- Skin-toned background
- Dim/flickering lights

### Signing Technique
âœ… **DO:**
- Exaggerate hand shapes
- Hold signs steady 2+ seconds
- Remove hand completely between signs
- Practice consistent positioning

âŒ **DON'T:**
- Rush between letters
- Move hand while detecting
- Overlap letters
- Use subtle finger movements

### Performance Optimization
```python
# For faster detection (lower accuracy):
STABLE_PREDICTIONS_REQUIRED = 7
MIN_HAND_FRAMES = 18

# For higher accuracy (slower):
STABLE_PREDICTIONS_REQUIRED = 15
MIN_HAND_FRAMES = 25
CONFIDENCE_THRESHOLD = 0.90
```

---

## ğŸ“š Educational Use

### For Students
- Computer Vision practical project
- Deep Learning implementation example
- Real-time system design
- Accessibility technology

### For Researchers
- Baseline for gesture recognition
- LSTM sequence modeling reference
- MediaPipe integration example
- Dataset preparation techniques

### For Developers
- Production-ready UI design
- Error handling best practices
- Real-time performance optimization
- Cross-platform deployment

---

## ğŸ™ Credits & Acknowledgments

### Dataset
- **Kaggle Sign Language Alphabets Dataset**
- Used for A-Z letter training
- ~10,000 images across 26 classes

### Core Technologies
- **MediaPipe** (Google): Hand tracking
- **TensorFlow** (Google): LSTM model
- **OpenCV** (Intel): Video processing
- **NumPy** (Community): Array operations

### Inspiration
- ASL recognition research papers
- MediaPipe gesture demos
- Sign language accessibility projects

---

## ğŸ“§ Support & Contact

**Developer**: [Mannankhan-sys]  
**Email**: muhammadabdulm64@gmail.com  
**University**: [NUML]  
**Project**: Final Year Project (FYP)  
**Supervisor**: [Supervisor Name]  

**Report Issues**: [GitHub/GitLab Issues Link]  
**Documentation**: [Project Wiki Link]  
**Demo Video**: [YouTube/Drive Link]  

---

## ğŸ“„ License

MIT License - See LICENSE file for details

```
Copyright (c) 2025 [Mannankhan-sys]

Permission is hereby granted, free of charge, to use, modify,
and distribute this software for educational and commercial purposes.
```

---

## ğŸŒŸ Project Impact

**Accessibility**: Helps bridge communication gap for hearing-impaired individuals

**Education**: Demonstrates practical AI/ML application

**Innovation**: Combines computer vision + deep learning + HCI

**Open Source**: Contributes to assistive technology research

---

**Made with â¤ï¸ for accessibility and inclusive communication**

**Last Updated**: February 2025  
**Version**: 2.0 Ultimate  
**Status**: Production Ready âœ…
